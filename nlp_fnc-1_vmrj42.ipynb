{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.5.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2021.3.17)\n",
      "Requirement already satisfied: tqdm in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.54.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2.25.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn numpy scipy nltk pandas transformers\n",
    "\n",
    "'''\n",
    "Ensure the fnc-1 dataset is located in the same directory as this notebook, in a folder called 'fnc-1', as follows:\n",
    "    - fnc-1\n",
    "        - ______.csv\n",
    "    - nlp_fnc-1_vmrj42.ipynb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score, accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys, os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "PUNCTUATION = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the dataset\n",
    "class Dataset():\n",
    "    def __init__(self, name='train', path='fnc-1', size=1, clean=True, balance=False, validation=False, related_only=False):\n",
    "        self.path = path\n",
    "\n",
    "        if not os.path.isfile('%s/%s_all_%s_%s.csv' % (path, name, 'processed' if clean else 'unprocessed', 'balanced' if balance else 'unbalanced')):\n",
    "            print('Generating')\n",
    "            bodies = name + '_bodies.csv'\n",
    "            stances = name + '_stances.csv'\n",
    "            self.articles = self.read(bodies)\n",
    "            self.stances = self.read(stances)\n",
    "\n",
    "            self.articles = pd.read_csv('%s/%s' % (path, bodies))\n",
    "            self.stances = pd.read_csv('%s/%s' % (path, stances))\n",
    "\n",
    "            pd.options.display.max_columns = None\n",
    "            self.data = pd.merge(self.articles, self.stances, how='right', on='Body ID')\n",
    "            for index, d in self.data.iterrows():\n",
    "                self.data.at[index, 'Body ID'] = int(d['Body ID'])\n",
    "                if clean:\n",
    "                    self.data.at[index, 'Headline'] = preprocess(d['Headline'])\n",
    "                    self.data.at[index, 'articleBody'] = preprocess(d['articleBody'])\n",
    "                    \n",
    "            self.data.to_csv('%s/%s_all_%s_%s.csv' % (path, name, 'processed' if clean else 'unprocessed', 'balanced' if balance else 'unbalanced'), index = False)\n",
    "            \n",
    "            if balance:\n",
    "                data = self.read('%s_all_%s_%s.csv' % (name, 'processed' if clean else 'unprocessed', 'balanced' if balance else 'unbalanced'))\n",
    "                n_related = sum([0 if d['Stance'] == 'unrelated' else 1 for d in data])\n",
    "                print(n_related)\n",
    "                dataset_size = len(data)\n",
    "                for _ in range(dataset_size - (2 * n_related)):\n",
    "                    n = len(data) - 1\n",
    "                    while True:\n",
    "                        if data[n]['Stance'] == 'unrelated':\n",
    "                            del data[n]\n",
    "                            break\n",
    "                        n -= 1\n",
    "                random.shuffle(data)\n",
    "                self.write('%s_all_%s_%s.csv' % (name, 'processed' if clean else 'unprocessed', 'balanced' if balance else 'unbalanced'), data)    \n",
    "            \n",
    "        self.data = self.read('%s_all_%s_%s.csv' % (name, 'processed' if clean else 'unprocessed', 'balanced' if balance else 'unbalanced'))\n",
    "        if related_only:\n",
    "            self.data = [d for d in self.data if d['Stance'] != 'unrelated']\n",
    "        self.data = self.data[:int(len(self.data) * size)]\n",
    "        all_headlines = [d['Headline'] for d in self.data]\n",
    "        all_bodies = [d['articleBody'] for d in self.data]\n",
    "        self.all_text = [' '.join([a,b]) for a,b in zip(all_headlines, all_bodies)]\n",
    "        \n",
    "        if validation:\n",
    "            self.validation = self.data[-3000:]\n",
    "            self.data = self.data[:-3000]\n",
    "        \n",
    "\n",
    "    def read(self, filename):\n",
    "        rows = []\n",
    "        with open(self.path + '/' + filename, 'r', encoding='utf-8') as table:\n",
    "            r = csv.DictReader(table)\n",
    "            for line in r:\n",
    "                rows.append(line)\n",
    "        return rows\n",
    "    \n",
    "    def write(self, filename, data):\n",
    "        print(list(data[0].keys()))\n",
    "        with open(self.path + '/' + filename, 'w', encoding='utf-8') as table:\n",
    "            w = csv.DictWriter(table, fieldnames = list(data[0].keys()))\n",
    "            w.writeheader()\n",
    "            for row in data:\n",
    "                w.writerow(row)\n",
    "    \n",
    "# Function to pre-process the sentences before Tf-idf vectorization\n",
    "def preprocess(data):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    import string\n",
    "    tokens = word_tokenize(data)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    table = str.maketrans('', '', PUNCTUATION)\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(token) for token in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x,y):\n",
    "    try:\n",
    "        if type(x) is np.ndarray: x = x.reshape(1, -1) # get rid of the warning\n",
    "        if type(y) is np.ndarray: y = y.reshape(1, -1)\n",
    "        d = cosine_similarity(x, y)\n",
    "        d = d[0][0]\n",
    "    except:\n",
    "        print(x)\n",
    "        print(y)\n",
    "        d = 0.\n",
    "    return d\n",
    "\n",
    "def euclidean_dist(x,y):\n",
    "    try:\n",
    "        if type(x) is np.ndarray: x = x.reshape(1, -1) # get rid of the warning\n",
    "        if type(y) is np.ndarray: y = y.reshape(1, -1)\n",
    "        d = euclidean_distances(x, y)\n",
    "        d = d[0][0]\n",
    "    except:\n",
    "        print(x)\n",
    "        print(y)\n",
    "        d = 0.\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tfidf(mode, classifier = 'svm'):\n",
    "    print('Configuration: %s, %s' % (mode, classifier))\n",
    "\n",
    "    # Load the pre-processed datasets\n",
    "    train_dataset = Dataset()\n",
    "    test_dataset = Dataset(name='competition_test')\n",
    "    print('Datasets loaded')\n",
    "    # Instantiate the vocabulary\n",
    "    vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df=2)\n",
    "    vec.fit(train_dataset.all_text + test_dataset.all_text)\n",
    "    vocab = vec.vocabulary_\n",
    "    print('Vocabulary initialized')\n",
    "\n",
    "    # Select the classifier model\n",
    "    if classifier == 'randforest':\n",
    "        model = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    elif classifier == 'svm':\n",
    "        model = SVC()\n",
    "    elif classifier == 'lr':\n",
    "        model = LogisticRegression(C=2, class_weight = 'balanced')\n",
    "\n",
    "    # Select the load/save functions, depending on the type of feature generated\n",
    "    if mode == 'sim':\n",
    "        ext = 'npy'\n",
    "        load = np.load\n",
    "        save = np.save\n",
    "    else:\n",
    "        ext = 'npz'\n",
    "        load = sparse.load_npz\n",
    "        save = sparse.save_npz\n",
    "\n",
    "    if os.path.isfile('features/tfidf_train_%s.%s' % (mode, ext)):\n",
    "        X_train = load('features/tfidf_train_%s.%s' % (mode, ext))\n",
    "        X_test = load('features/tfidf_test_%s.%s' % (mode, ext))\n",
    "    else:\n",
    "        # Combine the headline and body into a single sentence, and use that as the input\n",
    "        if mode == 'combine':\n",
    "            # Generate the Tf-idf features for both datasets\n",
    "            vecHB = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df=2, vocabulary=vocab)\n",
    "            tfidfHB = vecHB.fit_transform([' '.join([d['Headline'], d['articleBody']]) for d in train_dataset.data])\n",
    "            print('tfidfs for headlines + bodies generated ', tfidfHB.shape)\n",
    "            test_vecHB = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df=2, vocabulary=vocab)\n",
    "            test_tfidfHB = test_vecHB.fit_transform([' '.join([d['Headline'], d['articleBody']]) for d in test_dataset.data])\n",
    "            print('tfidfs for test headlines + bodies generated ', test_tfidfHB.shape)\n",
    "\n",
    "            X_train = tfidfHB\n",
    "            X_test = test_tfidfHB\n",
    "        else:\n",
    "            # Generate the Tf-idf features for both datasets, headlines and bodies separately\n",
    "            vecH = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df=2, vocabulary=vocab)\n",
    "            tfidfH = vecH.fit_transform([d['Headline'] for d in train_dataset.data])\n",
    "            print('tfidfs for headlines generated ', tfidfH.shape)\n",
    "            test_vecH = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df=2, vocabulary=vocab)\n",
    "            test_tfidfH = test_vecH.fit_transform([d['Headline'] for d in test_dataset.data])\n",
    "            print('tfidfs for test headlines generated ', test_tfidfH.shape)\n",
    "\n",
    "            vecB = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df=2, vocabulary=vocab)\n",
    "            tfidfB = vecB.fit_transform([d['articleBody'] for d in train_dataset.data])\n",
    "            print('tfidfs for bodies generated ', tfidfB.shape)\n",
    "            test_vecB = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df=2, vocabulary=vocab)\n",
    "            test_tfidfB = test_vecB.fit_transform([d['articleBody'] for d in test_dataset.data])\n",
    "            print('tfidfs for test bodies generated ', test_tfidfB.shape)\n",
    "\n",
    "            # Handle different feature modes separately\n",
    "            if mode == 'sim':\n",
    "                # Find some similarity metrics. In this case, we use cosine similarity and euclidean distance\n",
    "                cos_sim = np.asarray(list(map(cosine_sim, tfidfH, tfidfB)))[:, np.newaxis]\n",
    "                print('cos similarity between header and body tfidfs generated ', cos_sim.shape)\n",
    "                euclid_dist = np.asarray(list(map(euclidean_dist, tfidfH, tfidfB)))[:, np.newaxis]\n",
    "                print('euclidean distances between header and body tfidfs generated ', euclid_dist.shape)\n",
    "\n",
    "\n",
    "                test_cos_sim = np.asarray(list(map(cosine_sim, test_tfidfH, test_tfidfB)))[:, np.newaxis]\n",
    "                print('cos similarity between test header and body tfidfs generated ', test_cos_sim.shape)\n",
    "                test_euclid_dist = np.asarray(list(map(euclidean_dist, test_tfidfH, test_tfidfB)))[:, np.newaxis]\n",
    "                print('euclidean distances between test header and body tfidfs generated ', test_euclid_dist.shape)\n",
    "\n",
    "                X_train = np.concatenate((cos_sim, euclid_dist), axis = 1)\n",
    "                X_test = np.concatenate((test_cos_sim, test_euclid_dist), axis = 1)\n",
    "            elif mode == 'cat':\n",
    "                # Concatenate the two Tf-idf features\n",
    "                X_train = sparse.hstack([tfidfH, tfidfB])\n",
    "                X_test = sparse.hstack([test_tfidfH, test_tfidfB])\n",
    "\n",
    "        save('features/tfidf_train_%s.%s' % (mode, ext), X_train)\n",
    "        save('features/tfidf_test_%s.%s' % (mode, ext), X_test)\n",
    "\n",
    "\n",
    "    y_train = [0 if d['Stance'] == 'unrelated' else 1 for d in train_dataset.data]\n",
    "    y_test = [0 if d['Stance'] == 'unrelated' else 1 for d in test_dataset.data]\n",
    "\n",
    "    # Fit the model\n",
    "    if os.path.isfile('models/tfidf_%s_%s.pkl' % (classifier, mode)):\n",
    "        with open('models/tfidf_%s_%s.pkl' % (classifier, mode), 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        print('Classifier loaded')\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        print('Classifier trained')\n",
    "        with open('models/tfidf_%s_%s.pkl' % (classifier, mode), 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "    # Make some predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate BERT features one by one - the dataset is too large to generate features for all documents in one pass\n",
    "def generate_bert_features(dataset, tokenizer, model, device, split=False, mode='cls', dtype='train', sim=False):\n",
    "    headlines = [d['Headline'] for d in dataset]\n",
    "    bodies = [d['articleBody'] for d in dataset]\n",
    "    feats = []\n",
    "\n",
    "    # Tokenize the inputs separately if split = True, otherwise tokenize them together\n",
    "    if split:\n",
    "        h_encoding = tokenizer(headlines, return_tensors='pt', padding=True, truncation=True)\n",
    "        b_encoding = tokenizer(bodies, return_tensors='pt', padding=True, truncation=True)\n",
    "        print('Dataset encoded')\n",
    "        \n",
    "        # For every (headline,body) token sequence pair, generate the features\n",
    "        for h_inp, h_type, h_att, b_inp, b_type, b_att in zip(\n",
    "            h_encoding['input_ids'], h_encoding['token_type_ids'], h_encoding['attention_mask'], b_encoding['input_ids'], b_encoding['token_type_ids'], b_encoding['attention_mask']\n",
    "            ):\n",
    "            h_output = model(h_inp.to(device).unsqueeze(0), token_type_ids = h_type.to(device).unsqueeze(0), attention_mask = h_att.to(device).unsqueeze(0), output_hidden_states=True)\n",
    "            b_output = model(b_inp.to(device).unsqueeze(0), token_type_ids = b_type.to(device).unsqueeze(0), attention_mask = b_att.to(device).unsqueeze(0), output_hidden_states=True)\n",
    "\n",
    "            # Use the vector associated with the '[CLS]' tag\n",
    "            if mode == 'cls':\n",
    "                h_svec = h_output[0][0][0].squeeze().cpu().detach().numpy()\n",
    "                b_svec = b_output[0][0][0].squeeze().cpu().detach().numpy()\n",
    "            # Use the pooled output\n",
    "            elif mode == 'pooled':\n",
    "                h_svec = h_output[1][0].squeeze().cpu().detach().numpy()\n",
    "                b_svec = b_output[1][0].squeeze().cpu().detach().numpy()\n",
    "            # Concatenate the last 4 hidden layers, and find the mean to get a sentence vector\n",
    "            elif mode == 'cat':\n",
    "                h_cat = torch.cat([h_output[2][i] for i in [-1,-2,-3,-4]], dim=-1).squeeze()\n",
    "                b_cat = torch.cat([b_output[2][i] for i in [-1,-2,-3,-4]], dim=-1).squeeze()\n",
    "                h_svec = h_cat[0].cpu().detach().numpy()\n",
    "                b_svec = b_cat[0].cpu().detach().numpy()\n",
    "            # Either use cosine similarity as a feature, or join the two vectors into one feature\n",
    "            if sim:\n",
    "                feats += [[cosine_sim(h_svec, b_svec)]]\n",
    "            else:\n",
    "                feats += [np.append(h_svec, b_svec)]\n",
    "\n",
    "    else:\n",
    "        encoding = tokenizer(headlines, bodies, return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids, token_type_ids, attention_masks = encoding['input_ids'], encoding['token_type_ids'], encoding['attention_mask']\n",
    "\n",
    "        for input_id, token_type_id, attention_mask in zip(input_ids, token_type_ids, attention_masks):\n",
    "            output = model(input_id.to(device).unsqueeze(0), token_type_ids = token_type_id.to(device).unsqueeze(0), attention_mask = attention_mask.to(device).unsqueeze(0), output_hidden_states=True)\n",
    "\n",
    "            cls_feat = output[0].squeeze()[0]\n",
    "            pooled_output = output[1].squeeze()\n",
    "\n",
    "            hidden_states = output[2]\n",
    "            cat = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1).squeeze()\n",
    "            \n",
    "            # Use the '[CLS]' token representing the pair as a feature\n",
    "            if mode == 'cls':\n",
    "                feats += [cls_feat.cpu().detach().numpy()]\n",
    "            # Use the pooled output as a feature\n",
    "            elif mode == 'pooled':\n",
    "                feats += [pooled_output.cpu().detach().numpy()]\n",
    "            # Use the last 4 hidden layers, concatenated, as the feature\n",
    "            elif mode == 'cat':\n",
    "                feats += [cat[0].cpu().detach().numpy()]\n",
    "\n",
    "    np.save('features/%s_%s_%s_%s.npy' % (dtype, mode, 'sim' if sim else 'cat', 'split' if split else 'pair'), np.array(feats))\n",
    "    print('Features Generated')\n",
    "    return feats\n",
    "\n",
    "def classify_bert(mode, classifier = 'svm', sim = False, split = False):\n",
    "    print('Configuration: %s, %s, %s, %s' % (mode, classifier, 'cossim' if sim else 'stack', 'split' if split else 'paired'))\n",
    "\n",
    "    # Select the classifier model\n",
    "    if classifier == 'randforest':\n",
    "        model = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    elif classifier == 'svm':\n",
    "        model = SVC()\n",
    "    elif classifier == 'lr':\n",
    "        model = LogisticRegression(C=2, class_weight = 'balanced')\n",
    "\n",
    "    # Load the raw dataset, unprocessed\n",
    "    train_dataset = Dataset(clean=False, validation=False)\n",
    "    test_dataset = Dataset(name='competition_test', clean=False)\n",
    "    print('Datasets loaded')\n",
    "\n",
    "    # Instantiate the tokenizer and BERT model from a pretrained set of weights\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert = transformers.BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "    bert.eval()\n",
    "\n",
    "    # Generate the set of features for both datasets\n",
    "    with torch.no_grad():\n",
    "        if not os.path.isfile('features/train_%s_%s_%s.npy' % (mode, 'sim' if sim else 'cat', 'split' if split else 'pair')):\n",
    "            X_train = generate_bert_features(train_dataset.data, tokenizer, bert, device, mode=mode, split=split, sim=sim)\n",
    "        else:\n",
    "            X_train = np.load('features/train_%s_%s_%s.npy' % (mode, 'sim' if sim else 'cat', 'split' if split else 'pair'))\n",
    "        y_train = [0 if d['Stance'] == 'unrelated' else 1 for d in train_dataset.data]\n",
    "\n",
    "        if not os.path.isfile('features/test_%s_%s_%s.npy' % (mode, 'sim' if sim else 'cat', 'split' if split else 'pair')):\n",
    "            X_test = generate_bert_features(test_dataset.data, tokenizer, bert, device, mode=mode, dtype='test', split=split, sim=sim)\n",
    "        else:\n",
    "            X_test = np.load('features/test_%s_%s_%s.npy' % (mode, 'sim' if sim else 'cat', 'split' if split else 'pair'))\n",
    "        y_test = [0 if d['Stance'] == 'unrelated' else 1 for d in test_dataset.data]\n",
    "\n",
    "        # Fit the model\n",
    "        if os.path.isfile('models/bert_%s_%s_%s_%s.pkl' % (classifier, mode, 'sim' if sim else 'cat', 'split' if split else 'pair')):\n",
    "            with open('models/bert_%s_%s_%s_%s.pkl' % (classifier, mode, 'sim' if sim else 'cat', 'split' if split else 'pair'), 'rb') as file:\n",
    "                model = pickle.load(file)\n",
    "            print('Classifier loaded')\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            print('Classifier trained')\n",
    "            with open('models/bert_%s_%s_%s_%s.pkl' % (classifier, mode, 'sim' if sim else 'cat', 'split' if split else 'pair'), 'wb') as file:\n",
    "                pickle.dump(model, file)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: cls, randforest, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75     18349\n",
      "           1       0.33      0.32      0.32      7064\n",
      "\n",
      "    accuracy                           0.63     25413\n",
      "   macro avg       0.54      0.54      0.54     25413\n",
      "weighted avg       0.63      0.63      0.63     25413\n",
      "\n",
      "Configuration: pooled, randforest, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73     18349\n",
      "           1       0.29      0.28      0.28      7064\n",
      "\n",
      "    accuracy                           0.61     25413\n",
      "   macro avg       0.51      0.51      0.51     25413\n",
      "weighted avg       0.60      0.61      0.61     25413\n",
      "\n",
      "Configuration: cat, randforest, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.75     18349\n",
      "           1       0.34      0.33      0.33      7064\n",
      "\n",
      "    accuracy                           0.63     25413\n",
      "   macro avg       0.54      0.54      0.54     25413\n",
      "weighted avg       0.63      0.63      0.63     25413\n",
      "\n",
      "Configuration: cls, randforest, stack, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.98      0.84     18349\n",
      "           1       0.66      0.09      0.16      7064\n",
      "\n",
      "    accuracy                           0.73     25413\n",
      "   macro avg       0.70      0.54      0.50     25413\n",
      "weighted avg       0.72      0.73      0.65     25413\n",
      "\n",
      "Configuration: pooled, randforest, stack, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81     18349\n",
      "           1       0.40      0.20      0.26      7064\n",
      "\n",
      "    accuracy                           0.70     25413\n",
      "   macro avg       0.57      0.54      0.54     25413\n",
      "weighted avg       0.65      0.70      0.66     25413\n",
      "\n",
      "Configuration: cat, randforest, stack, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.98      0.84     18349\n",
      "           1       0.67      0.11      0.20      7064\n",
      "\n",
      "    accuracy                           0.74     25413\n",
      "   macro avg       0.70      0.55      0.52     25413\n",
      "weighted avg       0.72      0.74      0.66     25413\n",
      "\n",
      "Configuration: cls, randforest, cossim, paired\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     18349\n",
      "           1       0.96      0.96      0.96      7064\n",
      "\n",
      "    accuracy                           0.98     25413\n",
      "   macro avg       0.97      0.97      0.97     25413\n",
      "weighted avg       0.98      0.98      0.98     25413\n",
      "\n",
      "Configuration: pooled, randforest, cossim, paired\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     18349\n",
      "           1       0.96      0.96      0.96      7064\n",
      "\n",
      "    accuracy                           0.98     25413\n",
      "   macro avg       0.97      0.97      0.97     25413\n",
      "weighted avg       0.98      0.98      0.98     25413\n",
      "\n",
      "Configuration: cat, randforest, cossim, paired\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     18349\n",
      "           1       0.96      0.96      0.96      7064\n",
      "\n",
      "    accuracy                           0.98     25413\n",
      "   macro avg       0.97      0.97      0.97     25413\n",
      "weighted avg       0.98      0.98      0.98     25413\n",
      "\n",
      "Configuration: sim, randforest\n",
      "Datasets loaded\n",
      "Vocabulary initialized\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97     18349\n",
      "           1       0.92      0.91      0.91      7064\n",
      "\n",
      "    accuracy                           0.95     25413\n",
      "   macro avg       0.94      0.94      0.94     25413\n",
      "weighted avg       0.95      0.95      0.95     25413\n",
      "\n",
      "Configuration: combine, randforest\n",
      "Datasets loaded\n",
      "Vocabulary initialized\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82     18349\n",
      "           1       0.28      0.05      0.08      7064\n",
      "\n",
      "    accuracy                           0.70     25413\n",
      "   macro avg       0.50      0.50      0.45     25413\n",
      "weighted avg       0.60      0.70      0.62     25413\n",
      "\n",
      "Configuration: cat, randforest\n",
      "Datasets loaded\n",
      "Vocabulary initialized\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.99      0.84     18349\n",
      "           1       0.55      0.04      0.08      7064\n",
      "\n",
      "    accuracy                           0.72     25413\n",
      "   macro avg       0.64      0.52      0.46     25413\n",
      "weighted avg       0.68      0.72      0.63     25413\n",
      "\n",
      "Configuration: cls, lr, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69     18349\n",
      "           1       0.37      0.60      0.46      7064\n",
      "\n",
      "    accuracy                           0.61     25413\n",
      "   macro avg       0.59      0.61      0.58     25413\n",
      "weighted avg       0.68      0.61      0.63     25413\n",
      "\n",
      "Configuration: pooled, lr, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.39      0.51     18349\n",
      "           1       0.28      0.63      0.39      7064\n",
      "\n",
      "    accuracy                           0.46     25413\n",
      "   macro avg       0.51      0.51      0.45     25413\n",
      "weighted avg       0.61      0.46      0.48     25413\n",
      "\n",
      "Configuration: cat, lr, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.43      0.55     18349\n",
      "           1       0.30      0.62      0.40      7064\n",
      "\n",
      "    accuracy                           0.48     25413\n",
      "   macro avg       0.52      0.53      0.47     25413\n",
      "weighted avg       0.62      0.48      0.51     25413\n",
      "\n",
      "Configuration: cls, lr, stack, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.59      0.65     18349\n",
      "           1       0.29      0.43      0.34      7064\n",
      "\n",
      "    accuracy                           0.54     25413\n",
      "   macro avg       0.51      0.51      0.50     25413\n",
      "weighted avg       0.61      0.54      0.56     25413\n",
      "\n",
      "Configuration: pooled, lr, stack, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.63      0.68     18349\n",
      "           1       0.29      0.40      0.34      7064\n",
      "\n",
      "    accuracy                           0.57     25413\n",
      "   macro avg       0.51      0.51      0.51     25413\n",
      "weighted avg       0.61      0.57      0.58     25413\n",
      "\n",
      "Configuration: cat, lr, stack, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.55      0.63     18349\n",
      "           1       0.29      0.48      0.36      7064\n",
      "\n",
      "    accuracy                           0.53     25413\n",
      "   macro avg       0.51      0.52      0.50     25413\n",
      "weighted avg       0.61      0.53      0.56     25413\n",
      "\n",
      "Configuration: cls, lr, cossim, paired\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98     18349\n",
      "           1       0.94      0.97      0.96      7064\n",
      "\n",
      "    accuracy                           0.98     25413\n",
      "   macro avg       0.96      0.97      0.97     25413\n",
      "weighted avg       0.98      0.98      0.98     25413\n",
      "\n",
      "Configuration: pooled, lr, cossim, paired\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     18349\n",
      "           1       0.92      0.98      0.95      7064\n",
      "\n",
      "    accuracy                           0.97     25413\n",
      "   macro avg       0.96      0.97      0.96     25413\n",
      "weighted avg       0.97      0.97      0.97     25413\n",
      "\n",
      "Configuration: cat, lr, cossim, paired\n",
      "Datasets loaded\n",
      "Classifier loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98     18349\n",
      "           1       0.94      0.97      0.95      7064\n",
      "\n",
      "    accuracy                           0.97     25413\n",
      "   macro avg       0.96      0.97      0.97     25413\n",
      "weighted avg       0.97      0.97      0.97     25413\n",
      "\n",
      "Configuration: sim, lr\n",
      "Datasets loaded\n",
      "Vocabulary initialized\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     18349\n",
      "           1       0.95      0.93      0.94      7064\n",
      "\n",
      "    accuracy                           0.97     25413\n",
      "   macro avg       0.96      0.95      0.96     25413\n",
      "weighted avg       0.97      0.97      0.97     25413\n",
      "\n",
      "Configuration: combine, lr\n",
      "Datasets loaded\n",
      "Vocabulary initialized\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.09      0.16     18349\n",
      "           1       0.28      0.94      0.44      7064\n",
      "\n",
      "    accuracy                           0.32     25413\n",
      "   macro avg       0.54      0.51      0.30     25413\n",
      "weighted avg       0.65      0.32      0.23     25413\n",
      "\n",
      "Configuration: cat, lr\n",
      "Datasets loaded\n",
      "Vocabulary initialized\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.24      0.36     18349\n",
      "           1       0.29      0.80      0.42      7064\n",
      "\n",
      "    accuracy                           0.39     25413\n",
      "   macro avg       0.52      0.52      0.39     25413\n",
      "weighted avg       0.63      0.39      0.38     25413\n",
      "\n",
      "Configuration: cls, svm, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.97      0.84     18349\n",
      "           1       0.59      0.11      0.19      7064\n",
      "\n",
      "    accuracy                           0.73     25413\n",
      "   macro avg       0.66      0.54      0.52     25413\n",
      "weighted avg       0.70      0.73      0.66     25413\n",
      "\n",
      "Configuration: pooled, svm, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84     18349\n",
      "           1       0.00      0.00      0.00      7064\n",
      "\n",
      "    accuracy                           0.72     25413\n",
      "   macro avg       0.36      0.50      0.42     25413\n",
      "weighted avg       0.52      0.72      0.61     25413\n",
      "\n",
      "Configuration: cat, svm, cossim, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.99      0.85     18349\n",
      "           1       0.82      0.08      0.15      7064\n",
      "\n",
      "    accuracy                           0.74     25413\n",
      "   macro avg       0.78      0.54      0.50     25413\n",
      "weighted avg       0.76      0.74      0.65     25413\n",
      "\n",
      "Configuration: cls, svm, stack, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85     18349\n",
      "           1       0.71      0.20      0.31      7064\n",
      "\n",
      "    accuracy                           0.75     25413\n",
      "   macro avg       0.73      0.58      0.58     25413\n",
      "weighted avg       0.74      0.75      0.70     25413\n",
      "\n",
      "Configuration: pooled, svm, stack, split\n",
      "Datasets loaded\n",
      "Classifier loaded\n"
     ]
    }
   ],
   "source": [
    "# Training all our classifiers, in different configurations\n",
    "for classifier in ['randforest', 'lr', 'svm']:\n",
    "    classify_bert('cls', classifier, split=True, sim=True)\n",
    "    classify_bert('pooled', classifier, split=True, sim=True)\n",
    "    classify_bert('cat', classifier, split=True, sim=True)\n",
    "    \n",
    "    classify_bert('cls', classifier, split=True, sim=False)\n",
    "    classify_bert('pooled', classifier, split=True, sim=False)\n",
    "    classify_bert('cat', classifier, split=True, sim=False)\n",
    "    \n",
    "    classify_bert('cls', classifier, split=False, sim=True)\n",
    "    classify_bert('pooled', classifier, split=False, sim=True)\n",
    "    classify_bert('cat', classifier, split=False, sim=True)\n",
    "\n",
    "    classify_tfidf('sim', classifier)\n",
    "    classify_tfidf('combine', classifier)\n",
    "    classify_tfidf('cat', classifier)\n",
    "    # Using the 'combine' and 'cat' modes result in extremely long training times, and\n",
    "    # scores poorly, so they are not recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "lr = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Cls(nn.Module):\n",
    "    def __init__(self, output_dim, classes, mode):\n",
    "        super(BERT_Cls, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.classes = classes\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "            \n",
    "        if self.mode == 'lstm':\n",
    "            self.lstm = nn.LSTM(768, self.output_dim, batch_first = True, bidirectional = True)\n",
    "            self.output_dim = self.output_dim\n",
    "        \n",
    "        elif self.mode == 'linear':\n",
    "            self.cls = nn.Sequential(\n",
    "                nn.Linear(768 * 2, self.output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            )\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.output_dim = 768\n",
    "        \n",
    "        self.linear = nn.Linear(self.output_dim, self.classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        berts = self.bert(**tokens)\n",
    "        \n",
    "        if self.mode == 'lstm':\n",
    "            out, (hidden, cell) = self.lstm(berts.last_hidden_state)\n",
    "            hidden = self.dropout(hidden)\n",
    "            feat = torch.mean(hidden, 0).squeeze()\n",
    "            \n",
    "        elif self.mode == 'linear':\n",
    "            #feat = self.dropout(berts.pooler_output)\n",
    "            feat = self.dropout(berts.last_hidden_state[:,0])\n",
    "            #feat = self.cls(feat)\n",
    "            \n",
    "        \n",
    "        return self.linear(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = 'linear'\n",
    "mode = 'bert_related'\n",
    "\n",
    "model = BERT_Cls(64, 2, network).to(device)\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Use NLLLoss in conjunction with LogSoftmax for classification tasks\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Use Adam optimizer\n",
    "optimizer = transformers.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = Dataset(clean = False, balance = False, validation=True)\n",
    "valid_dataset = train_dataset.validation\n",
    "test_dataset = Dataset(name='competition_test', clean = False)\n",
    "\n",
    "# Load latest checkpointed model if available\n",
    "epoch = 0\n",
    "for i in range(epochs):\n",
    "    if os.path.isfile('models/%s_%s_epoch_%d.pth' % (network, mode, i)):\n",
    "        #model.load_state_dict(torch.load('models/%s_%s_epoch_%d.wgt' % (network, mode, i)))\n",
    "        epoch = i + 1\n",
    "        \n",
    "# Load tracked data for the model if available\n",
    "run_data = {'loss':[], 'valid_acc': [], 'valid_f1': []}\n",
    "if os.path.isfile('models/%s_%s.npy' % (network, mode)):\n",
    "    temp = np.load('models/%s_%s.npy' % (network, mode))\n",
    "    run_data['loss'] = list(temp[0])\n",
    "    run_data['valid_acc'] = list(temp[1])\n",
    "    run_data['valid_f1'] = list(temp[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F-Measure: 0.963885 \n",
      "Accuracy: 0.980667\n",
      "Validation accuracy and f1 have decreased. Ending training.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98     18349\n",
      "           1       0.95      0.97      0.96      7063\n",
      "\n",
      "    accuracy                           0.98     25412\n",
      "   macro avg       0.97      0.97      0.97     25412\n",
      "weighted avg       0.98      0.98      0.98     25412\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if epoch < epochs:\n",
    "    for e in range(epoch, epochs):\n",
    "        loss_total = 0\n",
    "        random.shuffle(train_dataset.data)\n",
    "        model.train()\n",
    "        for n, i in enumerate(range(0, len(train_dataset.data), batch_size)):\n",
    "            t = time.time()\n",
    "            # Get the batch\n",
    "            batch = train_dataset.data[i:min(i+batch_size, len(train_dataset.data))]\n",
    "            if len(batch) > 0:\n",
    "                #if network == 'linear':\n",
    "                tokens = tokenizer([d['Headline'] for d in batch],[d['articleBody'] for d in batch], return_tensors = 'pt', truncation = True, padding = True).to(device)\n",
    "                targets = torch.LongTensor([0 if d['Stance'] == 'unrelated' else 1 for d in batch]).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Score the sentence pairs, and calculate/update the loss\n",
    "                scores = model(tokens)\n",
    "                #print(scores.shape, targets.shape)\n",
    "                loss = loss_function(scores, targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                loss_total += loss.item()\n",
    "                optimizer.step()\n",
    "\n",
    "                t = time.time() - t\n",
    "                t = ((epochs - 1 - e) * (((len(train_dataset.data) // batch_size) + 1) * t)) + ((((len(train_dataset.data) - i - batch_size) // batch_size) + 1) * t)\n",
    "\n",
    "                sys.stdout.write('\\rEpoch: %d | Number of Trained Pairs: %d/%d | Avg Loss for Last Batch: %f | Remaining Time: %d:%d' % (\n",
    "                    e,\n",
    "                    i + batch_size,\n",
    "                    len(train_dataset.data),\n",
    "                    loss.item(),\n",
    "                    t // 60,\n",
    "                    int(t % 60)\n",
    "                    ))\n",
    "\n",
    "        run_data['loss'] += [loss_total / (n + 1)]\n",
    "        display.clear_output(wait = True)\n",
    "        plt.clf()\n",
    "        plt.ylabel('Avg. Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.plot(run_data['loss'], color = 'b')\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        # Test the network after every epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_arr, target_arr = [], []\n",
    "            for i in range(0, len(valid_dataset), batch_size):\n",
    "                batch = valid_dataset[i:min(i+batch_size, len(valid_dataset))]\n",
    "                if len(batch) > 0:\n",
    "                    tokens = tokenizer([d['Headline'] for d in batch],[d['articleBody'] for d in batch], return_tensors = 'pt', truncation = True, padding = True).to(device)\n",
    "                    targets = [0 if d['Stance'] == 'unrelated' else 1 for d in batch]\n",
    "\n",
    "                    scores = model(tokens)\n",
    "                    preds = torch.argmax(F.softmax(scores, dim=-1), dim=-1).cpu().numpy()\n",
    "\n",
    "                    pred_arr += list(preds)\n",
    "                    target_arr += targets\n",
    "\n",
    "            f1 = f1_score(target_arr, pred_arr)\n",
    "            run_data['valid_f1'] += [f1]\n",
    "            acc = accuracy_score(target_arr, pred_arr)\n",
    "            run_data['valid_acc'] += [acc]\n",
    "            print('\\nF-Measure: %3f' % f1, '\\nAccuracy: %3f' % acc)\n",
    "        \n",
    "        try:\n",
    "            if run_data['valid_acc'][-1] < run_data['valid_acc'][-2] and run_data['valid_f1'][-1] < run_data['valid_f1'][-2]:\n",
    "                print('Validation accuracy and f1 have decreased. Ending training.')\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Checkpoint after every epoch\n",
    "        torch.save(model.state_dict(), 'models/%s_%s_epoch_%d.pth' % (network, mode, e))\n",
    "        np.save('models/%s_%s.npy' % (network, mode), np.array([run_data['loss'], run_data['valid_acc'], run_data['valid_f1']]))\n",
    "        print(run_data['loss'])\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_arr, target_arr = [], []\n",
    "    for i in range(0, len(train_dataset.data), batch_size):\n",
    "        batch = test_dataset.data[i:min(i+batch_size, len(test_dataset.data) - 1)]\n",
    "        if len(batch) > 0:\n",
    "            tokens = tokenizer([d['Headline'] for d in batch], [d['articleBody'] for d in batch], return_tensors = 'pt', truncation = True, padding = True).to(device)\n",
    "            targets = [0 if d['Stance'] == 'unrelated' else 1 for d in batch]\n",
    "\n",
    "            scores = model(tokens)\n",
    "            preds = torch.argmax(F.softmax(scores, dim=-1), dim=-1).cpu().numpy()\n",
    "\n",
    "            pred_arr += list(preds)\n",
    "            target_arr += targets\n",
    "\n",
    "    #data = f1_score(target_arr, pred_arr)\n",
    "    #acc = accuracy_score(target_arr, pred_arr)\n",
    "    #print('\\nF-Measure: %3f' % data, '\\nAccuracy: %3f' % acc)\n",
    "\n",
    "    report = classification_report(target_arr, pred_arr)\n",
    "    print(report)\n",
    "\n",
    "        \n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tfidf_Cls(nn.Module):\n",
    "    def __init__(self, hidden_dim, classes):\n",
    "        super(Tfidf_Cls, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.fc1 = nn.Linear(41988, self.hidden_dim)\n",
    "        #self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim, classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, input_vec):\n",
    "        output = self.dropout(self.activation(self.fc1(input_vec)))\n",
    "        #output = self.dropout(self.activation(self.fc2(output)))\n",
    "        return self.fc3(output)\n",
    "    \n",
    "def get_tfidf_vec(headline, body, vocab):\n",
    "    headline_vecs, body_vecs = [], []\n",
    "    for h, b in zip(headline, body):\n",
    "        vec = TfidfVectorizer(ngram_range=(1,1), max_df=0.8, min_df=2, vocabulary = vocab)\n",
    "        headline_vecs += [vec.fit_transform([h]).toarray()]\n",
    "        body_vecs += [vec.transform([b]).toarray()]\n",
    "        \n",
    "    return torch.FloatTensor(headline_vecs).to(device), torch.FloatTensor(body_vecs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "network = 'linear'\n",
    "mode = 'tfidf'\n",
    "lr = 0.001\n",
    "\n",
    "model = Tfidf_Cls(64, 2).to(device)\n",
    "\n",
    "# Use NLLLoss in conjunction with LogSoftmax for classification tasks\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = Dataset(clean = True, balance = False, validation=True)\n",
    "valid_dataset = train_dataset.validation\n",
    "test_dataset = Dataset(name='competition_test', clean = True)\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,1), max_df=0.8, min_df=2)\n",
    "vec.fit(train_dataset.all_text + test_dataset.all_text)\n",
    "vocab = vec.vocabulary_\n",
    "\n",
    "# Load latest checkpointed model if available\n",
    "epoch = 0\n",
    "for i in range(epochs):\n",
    "    if os.path.isfile('models/%s_%s_epoch_%d.pth' % (network, mode, i)):\n",
    "        model.load_state_dict(torch.load('models/%s_%s_epoch_%d.pth' % (network, mode, i)))\n",
    "        epoch = i + 1\n",
    "        \n",
    "# Load tracked data for the model if available\n",
    "run_data = {'loss':[], 'valid_acc': [], 'valid_f1': []}\n",
    "if os.path.isfile('models/%s_%s.npy' % (network, mode)):\n",
    "    temp = np.load('models/%s_%s.npy' % (network, mode))\n",
    "    run_data['loss'] = list(temp[0])\n",
    "    run_data['valid_acc'] = list(temp[1])\n",
    "    run_data['valid_f1'] = list(temp[2])\n",
    "    \n",
    "print(epoch, len(run_data['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAakUlEQVR4nO3de5RU5Znv8e/T3WAAiQLdgAoGxlsGlQHSXqLGKJAMHqOgTjcYPaMuE0eNlzgub0fHMeRmNGNMjGYGNWqMBsGTY4gHj3MijCtZHpWmQaADBiSIeIlNVEQNcnvOH29VurrtS3Vbu3btvX+ftWp17V27i6draf96P3u/72vujoiIZFdV3AWIiEi8FAQiIhmnIBARyTgFgYhIxikIREQyribuAnqrtrbWx4wZE3cZIiKJsnTp0s3uXtfZa4kLgjFjxtDU1BR3GSIiiWJmL3f1mlpDIiIZpyAQEck4BYGISMYpCEREMk5BICKScQoCEZGMUxCIiGRcZoLg2WfhuuvirkJEpPJkJgiam+Hmm6GlJe5KREQqS2aC4PTTwQzmz4+7EhGRypKZIBg5Eo4/HubNi7sSEZHKkpkgAGhshNWr1R4SESmUqSDIt4d0ViAi0iZTQTByJHz+87pOICJSKFNBANDQoPaQiEihzAXB6adDVZXaQyIieZkLgvzdQ/Png3vc1YiIxC9zQQBqD4mIFIo0CMxsmpm9aGbrzOzabo47w8zczOqjrCcv3x7SRWMRkQiDwMyqgTuBk4BxwJlmNq6T4wYDlwPPRVVLR4WDy9QeEpGsi/KM4Ehgnbuvd/ftwFxgeifHfRP4HrAtwlo+orER1qxRe0hEJMog2A94pWB7U27fX5nZJGC0u//v7t7IzC4wsyYza2ptbS1JcWoPiYgEsV0sNrMq4Dbgyp6Odfc57l7v7vV1dXUl+fdHjAiDy9QeEpGsizIIXgVGF2yPyu3LGwwcBvyXmW0AjgYWlOuCMYS7h9QeEpGsizIIlgAHmdlYM+sPzAIW5F909y3uXuvuY9x9DPAscKq7N0VYUzsaXCYiEmEQuPtO4BLgSWA1MM/dW8xstpmdGtW/2xv59pAGl4lIltVE+ebuvhBY2GHfjV0ce0KUtXSloQEuvhhWrYLDD4+jAhGReGVyZHEh3T0kIlmX+SBQe0hEsi7zQQBtdw+tWhV3JSIi5acgQO0hEck2BQEaXCYi2aYgyGlshBdfVHtIRLJHQZCjwWUiklUKgpzhw+GEE3T3kIhkj4KgQEOD2kMikj0KggJqD4lIFikICqg9JCJZpCDoIN8eWrky7kpERMpDQdCBBpeJSNYoCDpQe0hEskZB0In84DK1h0QkCxQEnTjtNLWHRCQ7FASdyLeHNPeQiGSBgqALjY3whz+oPSQi6acg6EK+PaTBZSKSdgqCLgwfDieeqLuHRCT9FATdaGhQe0hE0k9B0A21h0QkCxQE3VB7SESyQEHQg3x7aMWKuCsREYmGgqAHmntIRNJOQdCDurrQHtLgMhFJKwVBERobYe1atYdEJJ0UBEU47TSorlZ7SETSSUFQhLo6zT0kIumlICiS2kMiklYKgiLl20MaXCYiaaMgKFL+7iENLhORtFEQ9EJDQ2gPvfBC3JWIiJSOgqAXdPeQiKSRgqAX1B4SkTRSEPSS2kMikjYKgl5Se0hE0kZB0Euae0hE0kZB0AeNjbBundpDIpIOkQaBmU0zsxfNbJ2ZXdvJ6xea2UozW25mvzOzcVHWUyoaXCYiaRJZEJhZNXAncBIwDjizk1/0D7v74e4+AbgFuC2qekqptlZ3D4lIekR5RnAksM7d17v7dmAuML3wAHd/t2BzEJCYX6tqD4lIWkQZBPsBrxRsb8rta8fMvmZmLxHOCC7r7I3M7AIzazKzptbW1kiK7S21h0QkLWK/WOzud7r7AcA1wA1dHDPH3evdvb6urq68BXahthYmT1Z7SESSL8ogeBUYXbA9KrevK3OBGRHWU3INDaE9tHx53JWIiPRdlEGwBDjIzMaaWX9gFrCg8AAzO6hg82RgbYT1lJwGl4lIGkQWBO6+E7gEeBJYDcxz9xYzm21mp+YOu8TMWsxsOfDPwDlR1ROFfHtIg8tEJMlqonxzd18ILOyw78aC55dH+e+XQ0MDXHBBaA9NnBh3NSIivRf7xeKkU3tIRJJOQfAxqT0kIkmnICiBxkZ46SXdPSQiyaQgKIEZMzS4TESSS0FQArW1MGWKBpeJSDIpCEqkoUHtIRFJJgVBiag9JCJJpSAoEbWHRCSpFAQllG8PLVsWdyUiIsVTEJSQBpeJSBL1KgjMbIiZjY+qmKQbNiy0hzS4TESSpMcgMLP/MrNPmtlQoBm428wSsaRkHBoaYP16tYdEJDmKOSPYK7ek5OnAz9z9KGBqtGUll9pDIpI0xQRBjZntAzQCj0dcT+KpPSQiSVNMEMwmrCmwzt2XmNnfkLAFZMqtsVHtIRFJjh6DwN3nu/t4d784t73e3c+IvrTk0uAyEUmSYi4W35K7WNzPzJ4ys1YzO7scxSXVsGEwdaoGl4lIMhTTGvpi7mLxl4ANwIHAVVEWlQb5u4eam+OuRESke0VdLM59PRmY7+5bIqwnNfLtId09JCKVrpggeNzM1gCfAZ4yszpgW7RlJZ/aQyKSFMVcLL4WOAaod/cdwPvA9KgLSwO1h0QkCYq5WNwPOBt4xMweBc4H/hx1YWkwYwbU1Kg9JCKVrZjW0E8IbaG7co9JuX3SAw0uE5EkKCYIjnD3c9x9Ue5xHnBE1IWlRWMj/PGPag+JSOUqJgh2mdkB+Y3cyOJd0ZWULvn2kAaXiUilKiYIrgIW52YhfRpYBFwZbVnpMXSoVi4TkcpWzF1DTwEHAZcBlwKHAEMjritV1B4SkUpW1MI07v6hu6/IPT4EfhBxXami9pCIVLK+LlVpJa0i5YYO1eAyEalcfQ0C/TrrpYaG0B5aujTuSkRE2usyCMxspZmt6OSxEhhRxhpTQYPLRKRS1XTz2pfKVkUG5NtD8+bBzTeDqbkmIhWiyzMCd3+5u0c5i0yLhgbYsEHtIRGpLH29RiB9oPaQiFQiBUEZDR0KX/gC3H8/bN4cdzUiIoGCoMy+8x145x047zzdSioilaFPQWBmN5W4jsyYMAG+/314/HH40Y/irkZEpO9nBLrc+TFccgmccgpcfTUsWxZ3NSKSdX0KAnf/dTHHmdk0M3vRzNaZ2bWdvP7PZvb73PiEp8zsU32pJ2nM4Kc/hdpamDkT3nsv7opEJMu6G0cAgJl11sDYAjS5+6+6+b5q4E7gC8AmYImZLXD33xcctoywBOYHZnYRcAswszc/QFLV1sJDD8HkyXDppXDffXFXJCJZVcwZwSeACcDa3GM8MAo438xu7+b7jgTWuft6d98OzKXDWsfuvtjdP8htPpt738w44QS44YZwF9HDD8ddjYhkVY9nBIRf/Me6+y4AM/sJ8FvgOGBlN9+3H/BKwfYm4Khujj8feKKIelLlxhth8WK48EI46ig44ICev0dEpJSKOSMYAuxZsD0IGJoLhg9LUYSZnQ3UA7d28foFZtZkZk2tra2l+CcrRk1NaBHV1MCsWbB9e9wViUjWFBMEtwDLzew+M7uf0Ne/1cwGAb/p5vteBUYXbI/K7WvHzKYC1wOn5tY6+Ah3n+Pu9e5eX1dXV0TJybL//nDPPdDUBNdfH3c1IpI15kWMajKzfQg9f4Al7v5aEd9TA/wBmEIIgCXAl929peCYicCjwDR3X1tMwfX19d7U1FTMoYlz0UXw7/8OTzwB06bFXY2IpImZLXX3+s5e6/GMwMx+DZwA/Mbdf1VMCAC4+07gEuBJYDUwz91bzGy2mZ2aO+xWQttpvpktN7MFxbx3Wt12Gxx2GJxzDrzxRtzViEhW9HhGYGafJ9zSeTLhr/q5wOPuvi368j4qzWcEAC0tcMQRcOyx8OSTUKVJQESkBD7WGYG7P+3uFwN/A/wH0Ai8WdoSJe/QQ+H22+E3vwlTUYiIRK2ovzfNbABwBnAhcATwQJRFZd1XvxrWLrj+enjuubirEZG0K+YawTxCj38y8GPgAHe/NOrCsswM5syB/faDM8+ELVvirkhE0qyYM4J7Cb/8L3T3xcAxZnZnxHVl3t57wy9+ARs3hsFmmrJaRKJSzDWCJ4HxZnaLmW0Avgmsibowgc9+FmbPhrlzNReRiESnyykmzOxg4MzcYzPwCOEuoxPLVJsA11wTLhxfemkIhr/927grEpG06e6MYA3husCX3P04d78D2FWesiSvuhp+/nMYODBMQbEtlpt2RSTNuguC04HXgcVmdreZTQGsPGVJoX33DTOUrlgRFrMRESmlLoPA3R9z91nAp4HFwNeB4Wb2EzP7Ypnqk5yTT4avfx3uuAMWZHr8tYiUWjEXi99394fd/RTCxHHLgGsir0w+4uabYeLEsPD9pk1xVyMiadGrCQzc/e3cTKBToipIurbHHuEOog8/hLPPhl26YiMiJaCZbBLm4IPhrrvg6afh29+OuxoRSQMFQQL94z+GM4JvfAN++9u4qxGRpFMQJNRdd8HYsXDWWfDWW3FXIyJJpiBIqMGDw/WCN96A88/XFBQi0ncKggSrr4fvfhceeyysbCYi0hcKgoS74oqwrOUVV8DKlXFXIyJJpCBIuKoqeOABGDIEZs6EDz6IuyIRSRoFQQoMHw4PPghr1oTRxyIivaEgSImpU8NMpXffDfPnx12NiCSJgiBFZs+Go44KS11u2BB3NSKSFAqCFOnXL6xq5h6WuNyxI+6KRCQJFAQpM3ZsWO/42WfhppvirkZEkkBBkEIzZ4ZBZt/9LixaFHc1IlLpFAQp9cMfwiGHhDmJWlvjrkZEKpmCIKUGDYJHHgnzEJ17LuzeHXdFIlKpFAQpNn48/Nu/wcKF4QxBRKQzCoKUu/himD49jDFYujTuakSkEikIUs4M7r0XRoyAWbNg69a4KxKRSqMgyIBhw+Chh2D9evja1+KuRkQqjYIgI44/Hv7lX8KcRA8+GHc1IlJJFAQZcsMN8LnPhesGa9fGXY2IVAoFQYbU1IQWUb9+YQqK7dvjrkhEKoGCIGNGj4b77gt3EJ17ri4ei4iCIJOmT4dvfSuseTx+PCxeHHdFIhInBUFGXX89/O53oV00eTJcdplWNxPJKgVBhh1zDLzwQgiBO+6ACRPgmWfirkpEyk1BkHEDB4bpJxYtChePP/c5uPpq2LYt7spEpFwUBALAiSfCypVh+upbb4XPfAaamuKuSkTKIdIgMLNpZvaima0zs2s7ef14M2s2s51m9g9R1iI9Gzw4LGrzxBOwZQscfTTceKNuMxVJu8iCwMyqgTuBk4BxwJlmNq7DYRuBc4GHo6pDem/atHB28OUvwze/GdZBXrEi7qpEJCpRnhEcCaxz9/Xuvh2YC0wvPMDdN7j7CkCz5VeYIUPgZz+Dxx6D116D+nr4zndg5864KxORUosyCPYDXinY3pTb12tmdoGZNZlZU6uW2yqr6dOhpQVOOy3ccnrssbBmTdxViUgpJeJisbvPcfd6d6+vq6uLu5zMqa0Nq53NnQvr1sHEiXDbbbBrV9yViUgpRBkErwKjC7ZH5fZJQs2cGc4OvvhFuPJKOOEEeOmluKsSkY8ryiBYAhxkZmPNrD8wC1gQ4b8nZTByZLhucP/94YLy+PFw111aE1kkySILAnffCVwCPAmsBua5e4uZzTazUwHM7Agz2wQ0AP9hZi1R1SOlYwbnnAOrVsFxx4XFbv7+72HjxrgrE5G+MHePu4Zeqa+v9yaNdKoY7mHswZVXQnU1/OAHcN55ISxEpHKY2VJ3r+/stURcLJbKZQb/9E+hTTRxYhiZfMop8PrrcVcmIsVSEEhJjB0b5ivKz1t06KHw8MPhjEFEKpuCQEqmqirMZLp8ORxyCJx1FjQ0gIZ+iFQ2BYGU3MEHh7UOvvc9+PWvw9nBL38Zd1Ui0hUFgUSiujpMZ710aVge84wz4Oyz4e23465MRDpSEEikDjsMnn0WvvGNMDr50ENh4cK4qxKRQgoCiVy/fmE66+efh2HD4OST4StfgXffjbsyEQEFgZTRxIlhsZvrroP77oPDD4ennoq7KhFREEhZ7bFHmM76mWdgwACYOjWMSr7nHvjzn+OuTiSbFAQSi6OOgmXL4KabwsR1X/0qjBgRQuHeexUKIuWkIJDYDBgA//qvsHZtuLvoqqvCNNdf+YpCQaScNNeQVBT3cKYwfz7Mmwfr14dbUadMgcZGmDEjXHAWkd7pbq4hBYFULIWCSOkoCCTxFAoiH4+CQFJFoSDSewoCSS2FgkhxFASSCQoFka4pCCRzFAoi7SkIJNMUCiIKApG/6ioUJk+G446DSZPCY599tO6ypIuCQKQThaHwq1/BmjVtS2uOGBEmycsHw6RJMGaMwkGSS0EgUoStW+GFF6C5OQREczO0tMCuXeH1vfduHw4TJ4bV2KqrYy1bpCjdBUFNuYsRqVSDB4f20HHHte3btg1WrmwLhuZm+PGP4cMPw+sDB8KECW3BMGkSjBsH/fvH8iOI9InOCER6aceO0EbKB0NzMyxfDu+9F17v3z+stVB49jB+fJhkTyQuag2JRGz37jBzaj4Y8mcQb70VXq+uhk9/uv01hwkT4JOfjLVsyRAFgUgM3GHjxvZtpeZmeP31tmMOPLB9W2n8eBg+HKo0QbyUmK4RiMTADD71qfCYMaNt/xtvtA+HJUvCrax5VVVQVxcCoZjHnnuW/UeTlFEQiJTZyJFw0knhkffWW+E6Q0sL/OlP8OabbY/nnw9ft27t/P0GDiw+NGproV+/svyYkiAKApEKMHRoGNQ2eXLXx/zlL9Da2j4kOj42bQpnGW++CTt3dv1vdRUUI0aEr3V1ITSGDFGbKgsUBCIJMWAA7L9/ePTEHd55p/vQePNNWLUqfM1f1O6oqipMv1Fb2xYOPT0fOLCkP7aUgYJAJIXMwl/zQ4bAIYf0fPyOHbB5c1tA/OlPYa3o1tawf/Pm8HzNmrbt3bs7f68BA7oPi47bw4ZBjX4TxUofv4jQr1+YX2mffYo7fvfucMZRGBJdPV+3Ljx/992u32/IkM7DY+jQ8Nree7d9zT/fay8FSKnoYxSRXquqCr+khw4N02wUY/v2toDoLjw2bICmprBvx47u33Pw4Pbh0NnXrvbtuafmjspTEIhIWfTvD/vuGx7FcIcPPoC33w5nH/mvhc87fn355XD31TvvdH8GAmGQX7HBMXgwDBoUwmPQoPaPPfZIfqAoCESkIpm1/bIdNar3379zZwiDzgKjq32vvNK2Lz+fVE+qq9sHQ8ew6Cw8ij2mXLf6KghEJJVqatraV32xbVtbKGzdCu+/Hx7vvdf2vLvtLVvgtdfav/7BB72roV+/9sFw000wa1bffp7uKAhERDrxiU+EwX8jR5buPXfvDmFQTJB0th3VSnoKAhGRMqmqCn/ZV9q0IBozKCKScZEGgZlNM7MXzWydmV3byet7mNkjudefM7MxUdYjIiIfFVkQmFk1cCdwEjAOONPMxnU47HzgbXc/EPgB8L2o6hERkc5FeUZwJLDO3de7+3ZgLjC9wzHTgQdyzx8Fppgl/Y5cEZFkiTII9gNeKdjelNvX6THuvhPYAnzkuriZXWBmTWbW1NraGlG5IiLZlIiLxe4+x93r3b2+rq4u7nJERFIlyiB4FRhdsD0qt6/TY8ysBtgL+HOENYmISAdRBsES4CAzG2tm/YFZwIIOxywAzsk9/wdgkSdtEWURkYSLdPF6M/tvwO1ANfBTd/+2mc0Gmtx9gZl9AngQmAi8Bcxy9/U9vGcr8HIfS6oFNvfxe9NIn0d7+jza6LNoLw2fx6fcvdPeeqRBUGnMrMnd6+Ouo1Lo82hPn0cbfRbtpf3zSMTFYhERiY6CQEQk47IWBHPiLqDC6PNoT59HG30W7aX688jUNQIREfmorJ0RiIhIBwoCEZGMy0wQ9DQldlaY2WgzW2xmvzezFjO7PO6aKoGZVZvZMjN7PO5a4mZme5vZo2a2xsxWm9ln464pLmZ2Re7/k1Vm9ovc2KfUyUQQFDkldlbsBK5093HA0cDXMvxZFLocWB13ERXih8D/cfdPA39HRj8XM9sPuAyod/fDCANjI1gxOH6ZCAKKmxI7E9z9dXdvzj3fSvifvOOssJliZqOAk4F74q4lbma2F3A8cC+Au29393diLSpeNcCA3FxoA4HXYq4nElkJgmKmxM6c3IpwE4HnYi4lbrcDVwO7Y66jEowFWoH7cq2ye8xsUNxFxcHdXwW+D2wEXge2uPt/xltVNLISBNKBme0J/E/g6+7+btz1xMXMvgS86e5L466lQtQAk4CfuPtE4H0gk9fUzGwIoXMwFtgXGGRmZ8dbVTSyEgTFTImdGWbWjxACD7n7L+OuJ2bHAqea2QZCy3Cymf083pJitQnY5O75s8RHCcGQRVOBP7p7q7vvAH4JHBNzTZHIShAUMyV2JuSWAr0XWO3ut8VdT9zc/Tp3H+XuYwj/XSxy91T+1VcMd38DeMXMDsntmgL8PsaS4rQRONrMBub+v5lCSi+c18RdQDm4+04zuwR4krYpsVtiLisuxwL/HVhpZstz+/6Huy+MrySpMJcCD+X+aFoPnBdzPbFw9+fM7FGgmXC33TJSOtWEppgQEcm4rLSGRESkCwoCEZGMUxCIiGScgkBEJOMUBCIiGacgEOnAzHaZ2fKCR8lG1prZGDNbVar3EymFTIwjEOmlv7j7hLiLECkXnRGIFMnMNpjZLWa20syeN7MDc/vHmNkiM1thZk+Z2f65/SPM7H+Z2Qu5R356gmozuzs3z/1/mtmA2H4oERQEIp0Z0KE1NLPgtS3ufjjwY8KspQB3AA+4+3jgIeBHuf0/Ap52978jzNeTH81+EHCnux8KvAOcEelPI9IDjSwW6cDM3nP3PTvZvwGY7O7rcxP3veHuw8xsM7CPu+/I7X/d3WvNrBUY5e4fFrzHGOD/uvtBue1rgH7u/q0y/GgindIZgUjveBfPe+PDgue70LU6iZmCQKR3ZhZ8/X+558/QtoThWcBvc8+fAi6Cv66JvFe5ihTpDf0lIvJRAwpmZoWwfm/+FtIhZraC8Ff9mbl9lxJW9LqKsLpXfrbOy4E5ZnY+4S//iwgrXYlUFF0jEClS7hpBvbtvjrsWkVJSa0hEJON0RiAiknE6IxARyTgFgYhIxikIREQyTkEgIpJxCgIRkYz7/3/ce37vJEi3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F-Measure: 0.990548 \n",
      "Accuracy: 0.995000\n",
      "Validation accuracy and f1 have decreased. Ending training.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.37      0.54     18349\n",
      "           1       0.37      0.97      0.54      7063\n",
      "\n",
      "    accuracy                           0.54     25412\n",
      "   macro avg       0.67      0.67      0.54     25412\n",
      "weighted avg       0.80      0.54      0.54     25412\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if epoch < epochs:\n",
    "    for e in range(epoch, epochs):\n",
    "        loss_total = 0\n",
    "        random.shuffle(train_dataset.data)\n",
    "        model.train()\n",
    "        for n, i in enumerate(range(0, len(train_dataset.data), batch_size)):\n",
    "            t = time.time()\n",
    "            # Get the batch\n",
    "            batch = train_dataset.data[i:min(i+batch_size, len(train_dataset.data))]\n",
    "            if len(batch) > 0:\n",
    "                headlines, bodies = get_tfidf_vec([d['Headline'] for d in batch], [d['articleBody'] for d in batch], vocab)\n",
    "                targets = torch.LongTensor([0 if d['Stance'] == 'unrelated' else 1 for d in batch]).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Score the sentence pairs, and calculate/update the loss\n",
    "                scores = model(torch.cat((headlines, bodies), dim=-1).squeeze())\n",
    "                loss = loss_function(scores, targets)\n",
    "                loss.backward()\n",
    "                loss_total += loss.item()\n",
    "                optimizer.step()\n",
    "\n",
    "                t = time.time() - t\n",
    "                t = ((epochs - 1 - e) * (((len(train_dataset.data) // batch_size) + 1) * t)) + ((((len(train_dataset.data) - i - batch_size) // batch_size) + 1) * t)\n",
    "\n",
    "                sys.stdout.write('\\rEpoch: %d | Number of Trained Pairs: %d/%d | Avg Loss for Last Batch: %f | Remaining Time: %d:%d' % (\n",
    "                    e,\n",
    "                    i + batch_size,\n",
    "                    len(train_dataset.data),\n",
    "                    loss.item(),\n",
    "                    t // 60,\n",
    "                    int(t % 60)\n",
    "                    ))\n",
    "\n",
    "        run_data['loss'] += [loss_total / (n + 1)]\n",
    "        display.clear_output(wait = True)\n",
    "        plt.clf()\n",
    "        plt.ylabel('Avg. Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.plot(run_data['loss'], color = 'b')\n",
    "        #plt.savefig('loss_plot.png' % model_name)\n",
    "        display.display(plt.gcf())\n",
    "        \n",
    "        # Test the network after every epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_arr, target_arr = [], []\n",
    "            for i in range(0, len(valid_dataset), batch_size):\n",
    "                batch = valid_dataset[i:min(i+batch_size, len(valid_dataset))]\n",
    "                if len(batch) > 0:\n",
    "                    headlines, bodies = get_tfidf_vec([d['Headline'] for d in batch], [d['articleBody'] for d in batch], vocab)\n",
    "                    targets = [0 if d['Stance'] == 'unrelated' else 1 for d in batch]\n",
    "\n",
    "                    scores = model(torch.cat((headlines, bodies), dim=-1).squeeze())\n",
    "                    preds = torch.argmax(F.softmax(scores, dim=-1), dim=-1).cpu().numpy()\n",
    "\n",
    "                    pred_arr += list(preds)\n",
    "                    target_arr += targets\n",
    "\n",
    "            #print(pred_arr)\n",
    "            f1 = f1_score(target_arr, pred_arr)\n",
    "            run_data['valid_f1'] += [f1]\n",
    "            acc = accuracy_score(target_arr, pred_arr)\n",
    "            run_data['valid_acc'] += [acc]\n",
    "            print('\\nF-Measure: %3f' % f1, '\\nAccuracy: %3f' % acc)\n",
    "        \n",
    "        try:\n",
    "            if run_data['valid_acc'][-1] < run_data['valid_acc'][-2] and run_data['valid_f1'][-1] < run_data['valid_f1'][-2]:\n",
    "                print('Validation accuracy and f1 have decreased. Ending training.')\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Checkpoint after every epoch\n",
    "        torch.save(model.state_dict(), 'models/%s_%s_epoch_%d.pth' % (network, mode, e))\n",
    "        np.save('models/%s_%s.npy' % (network, mode), np.array([run_data['loss'], run_data['valid_acc'], run_data['valid_f1']]))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_arr, target_arr = [], []\n",
    "    for i in range(0, len(train_dataset.data), batch_size):\n",
    "        batch = test_dataset.data[i:min(i+batch_size, len(test_dataset.data) - 1)]\n",
    "        if len(batch) > 0:\n",
    "            headlines, bodies = get_tfidf_vec([d['Headline'] for d in batch], [d['articleBody'] for d in batch], vocab)\n",
    "            targets = [0 if d['Stance'] == 'unrelated' else 1 for d in batch]\n",
    "\n",
    "            scores = model(torch.cat((headlines, bodies), dim=-1).squeeze())\n",
    "            preds = torch.argmax(F.softmax(scores, dim=-1), dim=-1).cpu().numpy()\n",
    "\n",
    "            pred_arr += list(preds)\n",
    "            target_arr += targets\n",
    "\n",
    "    #print(pred_arr)\n",
    "    #data = f1_score(target_arr, pred_arr)\n",
    "    #acc = accuracy_score(target_arr, pred_arr)\n",
    "    #print('\\nF-Measure: %3f' % data, '\\nAccuracy: %3f' % acc)\n",
    "\n",
    "    report = classification_report(target_arr, pred_arr)\n",
    "    print(report)\n",
    "\n",
    "        \n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = 'lstm'\n",
    "mode = 'bert_sentiment'\n",
    "\n",
    "model = BERT_Cls(64, 3, network).to(device)\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Use NLLLoss in conjunction with LogSoftmax for classification tasks\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Use Adam optimizer\n",
    "optimizer = transformers.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = Dataset(clean = False, balance = False, related_only = True, validation=True)\n",
    "valid_dataset = train_dataset.validation\n",
    "test_dataset = Dataset(name='competition_test', clean = False, related_only = True)\n",
    "\n",
    "# Load latest checkpointed model if available\n",
    "epoch = 0\n",
    "for i in range(epochs):\n",
    "    if os.path.isfile('models/%s_%s_epoch_%d.pth' % (network, mode, i)):\n",
    "        model.load_state_dict(torch.load('models/%s_%s_epoch_%d.pth' % (network, mode, i)))\n",
    "        epoch = i + 1\n",
    "        \n",
    "# Load tracked data for the model if available\n",
    "run_data = {'loss':[], 'valid_acc': []}\n",
    "if os.path.isfile('models/%s_%s.npy' % (network, mode)):\n",
    "    temp = np.load('models/%s_%s.npy' % (network, mode))\n",
    "    run_data['loss'] = list(temp[0])\n",
    "    run_data['valid_acc'] = list(temp[1])\n",
    "    \n",
    "stances = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.63      0.58      1902\n",
      "           1       0.47      0.08      0.14       697\n",
      "           2       0.79      0.83      0.81      4464\n",
      "\n",
      "    accuracy                           0.70      7063\n",
      "   macro avg       0.60      0.51      0.51      7063\n",
      "weighted avg       0.69      0.70      0.68      7063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if epoch < epochs:\n",
    "    for e in range(epoch, epochs):\n",
    "        loss_total = 0\n",
    "        random.shuffle(train_dataset.data)\n",
    "        model.train()\n",
    "        for n, i in enumerate(range(0, len(train_dataset.data), batch_size)):\n",
    "            t = time.time()\n",
    "            # Get the batch\n",
    "            batch = train_dataset.data[i:min(i+batch_size, len(train_dataset.data))]\n",
    "            if len(batch) > 0:\n",
    "                #if network == 'linear':\n",
    "                tokens = tokenizer([d['Headline'] for d in batch],[d['articleBody'] for d in batch], return_tensors = 'pt', truncation = True, padding = True).to(device)\n",
    "                targets = torch.LongTensor([stances[d['Stance']] for d in batch]).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Score the sentence pairs, and calculate/update the loss\n",
    "                scores = model(tokens)\n",
    "                #print(scores.shape, targets.shape)\n",
    "                loss = loss_function(scores, targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                loss_total += loss.item()\n",
    "                optimizer.step()\n",
    "\n",
    "                t = time.time() - t\n",
    "                t = ((epochs - 1 - e) * (((len(train_dataset.data) // batch_size) + 1) * t)) + ((((len(train_dataset.data) - i - batch_size) // batch_size) + 1) * t)\n",
    "\n",
    "                sys.stdout.write('\\rEpoch: %d | Number of Trained Pairs: %d/%d | Avg Loss for Last Batch: %f | Remaining Time: %d:%d' % (\n",
    "                    e,\n",
    "                    i + batch_size,\n",
    "                    len(train_dataset.data),\n",
    "                    loss.item(),\n",
    "                    t // 60,\n",
    "                    int(t % 60)\n",
    "                    ))\n",
    "\n",
    "        run_data['loss'] += [loss_total / (n + 1)]\n",
    "        display.clear_output(wait = True)\n",
    "        plt.clf()\n",
    "        plt.ylabel('Avg. Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.plot(run_data['loss'], color = 'b')\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        # Test the network after every epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_arr, target_arr = [], []\n",
    "            for i in range(0, len(valid_dataset), batch_size):\n",
    "                batch = valid_dataset[i:min(i+batch_size, len(valid_dataset))]\n",
    "                if len(batch) > 0:\n",
    "                    tokens = tokenizer([d['Headline'] for d in batch],[d['articleBody'] for d in batch], return_tensors = 'pt', truncation = True, padding = True).to(device)\n",
    "                    targets = [stances[d['Stance']] for d in batch]\n",
    "\n",
    "                    scores = model(tokens)\n",
    "                    preds = torch.argmax(F.softmax(scores, dim=-1), dim=-1).cpu().numpy()\n",
    "\n",
    "                    pred_arr += list(preds)\n",
    "                    target_arr += targets\n",
    "\n",
    "            acc = accuracy_score(target_arr, pred_arr)\n",
    "            run_data['valid_acc'] += [acc]\n",
    "            print('\\nAccuracy: %3f' % acc)\n",
    "        \n",
    "        try:\n",
    "            if run_data['valid_acc'][-1] < run_data['valid_acc'][-2]:\n",
    "                print('Validation accuracy and f1 have decreased. Ending training.')\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Checkpoint after every epoch\n",
    "        torch.save(model.state_dict(), 'models/%s_%s_epoch_%d.pth' % (network, mode, e))\n",
    "        np.save('models/%s_%s.npy' % (network, mode), np.array([run_data['loss'], run_data['valid_acc']]))\n",
    "        print(run_data)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_arr, target_arr = [], []\n",
    "    for i in range(0, len(train_dataset.data), batch_size):\n",
    "        batch = test_dataset.data[i:min(i+batch_size, len(test_dataset.data) - 1)]\n",
    "        if len(batch) > 0:\n",
    "            tokens = tokenizer([d['Headline'] for d in batch], [d['articleBody'] for d in batch], return_tensors = 'pt', truncation = True, padding = True).to(device)\n",
    "            targets = [stances[d['Stance']] for d in batch]\n",
    "\n",
    "            scores = model(tokens)\n",
    "            preds = torch.argmax(F.softmax(scores, dim=-1), dim=-1).cpu().numpy()\n",
    "\n",
    "            pred_arr += list(preds)\n",
    "            target_arr += targets\n",
    "\n",
    "    #data = f1_score(target_arr, pred_arr)\n",
    "    #acc = accuracy_score(target_arr, pred_arr)\n",
    "    #print('\\nAccuracy: %3f' % acc)\n",
    "\n",
    "    report = classification_report(target_arr, pred_arr)\n",
    "    print(report)\n",
    "\n",
    "        \n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "classifier config:\n",
    "{\n",
    "    embedder: str,\n",
    "    related_params: list,\n",
    "    related_name: str,\n",
    "    sentiment_params: list,\n",
    "    sentiment_name: str,\n",
    "}\n",
    "'''\n",
    "\n",
    "class StanceClassifier():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        if self.config['embedder'] == 'bert':\n",
    "            self.model = BERT_Cls\n",
    "            self.tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        elif self.config['embedder'] == 'tfidf':\n",
    "            self.model = Tfidf_Cls\n",
    "            \n",
    "        self.relation = self.model(*self.config['related_params']).to(device)\n",
    "        self.sentiment = self.model(*self.config['sentiment_params']).to(device)\n",
    "        self.relation.load_state_dict(torch.load(self.config['related_name']))\n",
    "        self.sentiment.load_state_dict(torch.load(self.config['sentiment_name']))\n",
    "        self.relation.eval()\n",
    "        self.sentiment.eval()\n",
    "        \n",
    "    def predict(self, headline, body, vocab = None):\n",
    "        with torch.no_grad():\n",
    "            if self.config['embedder'] == 'bert':\n",
    "                tokens = self.tokenizer(headline, body, return_tensors = 'pt', truncation = True, padding = True).to(device)\n",
    "            elif self.config['embedder'] == 'tfidf':\n",
    "                tokens = get_tfidf_vec([headline], [body], vocab)\n",
    "                tokens = torch.cat(tokens, dim=-1)\n",
    "\n",
    "            scores = self.relation(tokens).unsqueeze(0)\n",
    "            pred = torch.argmax(F.softmax(scores, dim=-1), dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "            # Stances are: 0-agree, 1-disagree, 2-discuss, 3-unrelated\n",
    "            if pred == 0:\n",
    "                return 3\n",
    "\n",
    "            scores = self.sentiment(tokens).unsqueeze(0)\n",
    "            pred = torch.argmax(F.softmax(scores, dim=-1), dim=-1).squeeze().cpu().numpy()\n",
    "        \n",
    "        return pred        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91      3678\n",
      "           1       0.80      0.69      0.74       840\n",
      "           2       0.95      0.98      0.96      8909\n",
      "           3       1.00      0.99      0.99     36545\n",
      "\n",
      "    accuracy                           0.98     49972\n",
      "   macro avg       0.92      0.89      0.90     49972\n",
      "weighted avg       0.98      0.98      0.98     49972\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.57      0.58      1903\n",
      "           1       0.41      0.10      0.16       697\n",
      "           2       0.74      0.85      0.79      4464\n",
      "           3       0.99      0.98      0.99     18349\n",
      "\n",
      "    accuracy                           0.91     25413\n",
      "   macro avg       0.68      0.63      0.63     25413\n",
      "weighted avg       0.90      0.91      0.90     25413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'embedder': 'bert',\n",
    "    'related_params': [64, 2, 'lstm'],\n",
    "    'related_name': 'models/lstm_bert_related_epoch_9.pth',\n",
    "    'sentiment_params': [64, 3, 'lstm'],\n",
    "    'sentiment_name': 'models/lstm_bert_sentiment_epoch_8.pth',\n",
    "    'dataset_config': [False, False, False, False]\n",
    "}\n",
    "\n",
    "try:\n",
    "    del classifier\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "classifier = StanceClassifier(config)\n",
    "\n",
    "if config['embedder'] == 'bert':\n",
    "    clean = False\n",
    "elif config['embedder'] == 'tfidf':\n",
    "    clean = True\n",
    "    \n",
    "train_dataset = Dataset(clean=clean, validation=False)\n",
    "test_dataset = Dataset(name='competition_test', clean=clean, validation=False)\n",
    "\n",
    "if config['embedder'] == 'tfidf':\n",
    "    vec = TfidfVectorizer(ngram_range=(1,1), max_df=0.8, min_df=2)\n",
    "    vec.fit(train_dataset.all_text + test_dataset.all_text)\n",
    "    vocab = vec.vocabulary_\n",
    "else:\n",
    "    vocab = None\n",
    "    \n",
    "train_truths = [stances[d['Stance']] for d in train_dataset.data]\n",
    "train_preds = []\n",
    "for d in train_dataset.data:\n",
    "    train_preds += [classifier.predict(d['Headline'], d['articleBody'], vocab)]\n",
    "    \n",
    "report = classification_report(train_truths, train_preds)\n",
    "print(report)    \n",
    "\n",
    "test_truths = [stances[d['Stance']] for d in test_dataset.data]\n",
    "test_preds = []\n",
    "for d in test_dataset.data:\n",
    "    test_preds += [classifier.predict(d['Headline'], d['articleBody'], vocab)]\n",
    "    \n",
    "report = classification_report(test_truths, test_preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
